<!DOCTYPE HTML>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML" async></script>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

  <title>FETNet: Feature erasing and transferring network for scene text removal</title>
  
  <meta name="author" content="Ruiqi Wu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="../projects.css">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="icon" type="image/png" href="../../images/logo.png">
</head>

<body style="width: 100%;">
    <div style="width: 100%;">
        <!-- Title -->
        <div class="root-content" style="padding-top: 30px;">
            <name>FETNet: Feature Erasing and Transferring Network for Scene Text Removal</name>
            <br>
            <div id="author-list"">
                <a href="https://scholar.google.com/citations?hl=en&user=5923KIEAAAAJ">Guangtao Lyu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
                Kun Liu<sup>1</sup>&nbsp;&nbsp;&nbsp;

<!--                <a href="https://scholar.google.com/citations?user=H5pImFUAAAAJ&hl=en">Anna Zhu</a><sup>1,<a href="annazhu@whut.edu.cn"><img class="email-icon" src="../../images/email.ico" alt="email"></a></sup>&nbsp;&nbsp;&nbsp;-->
        <a href="https://scholar.google.com/citations?user=H5pImFUAAAAJ&hl=en">Anna Zhu</a><sup>1,
          <a href="#" onclick="toggleEmail(event); return false;">
            <img class="email-icon" src="../../images/email.ico" alt="email">
          </a>
          <span id="email-address" style="display: none;">annazhu@whut.edu.cn</span>
        </sup>&nbsp;&nbsp;&nbsp;

        <script>
          function toggleEmail(event) {
            var emailElement = document.getElementById('email-address');
            if (emailElement.style.display === 'none') {
              emailElement.style.display = 'inline';
            } else {
              emailElement.style.display = 'none';
            }
          }

          var emailLink = document.getElementById('email-address');
          emailLink.addEventListener('click', function() {
            var emailAddress = emailLink.textContent;
            var subject = 'Regarding your research paper';
            var body = 'Dear Anna Zhu,\n\nI am writing to inquire about your research paper.\n\nKind regards,\n[Your Name]';

            var mailtoUrl = 'mailto:' + encodeURIComponent(emailAddress) +
              '?subject=' + encodeURIComponent(subject) +
              '&body=' + encodeURIComponent(body);

            window.location.href = mailtoUrl;
          });
        </script>


                <a href="https://scholar.google.com/citations?hl=en&user=QMpdhysAAAAJ"> Seiichi Uchida</a><sup>2</sup>&nbsp;&nbsp;&nbsp;
                <a href="https://scholar.google.com/citations?hl=en&user=azIV5VkAAAAJ">  Brian Kenji Iwana</a><sup>2</sup>

            </div>
            <div id="institution-list">
                <sup>1</sup>School of Computer Science and Artificial Intelligence, Wuhan University of Technology, Wuhan, China
                <sup>2</sup>Human Interface Laboratory, Kyushu University, Fukuoka, Japan
            <p id="publication">
                PR 2023&nbsp;<br>
            </div>
            </p>
            <div id="button-list">
                <span class="link-button">
                    <a class="link-button-content", href="https://arxiv.org/abs/2306.09593", target="_blank">
                        <span>
                            <svg class="svg-inline--fa fa-file-pdf fa-w-12" style="position: relative; top: 0.15em;" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg>
                        </span>
                        &nbsp;
                        Paper
                    </a>
                </span>
                <span class="link-button">
                    <a class="link-button-content" href="https://github.com/GuangtaoLyu/FETNet", target="_blank">
                        <span>
                            <svg class="svg-inline--fa fa-file-pdf" aria-hidden="true" version="1.1" viewBox="0 0 16 16" width="1.2em" style="position: relative; top: 0.25em;"><path fill="currentColor" fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg>
                        </span>
                        &nbsp; 
                        Code
                    </a>
                </span>
                <span class="link-button">
                    <a class="link-button-content">
                        <span>
                            <svg class="svg-inline--fa fa-file-pdf" aria-hidden="true" style="position: relative; top: 0.25em;" width="1.2em"  viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Google Colab</title><path fill="currentColor" fill-rule="evenodd" d="M16.9414 4.9757a7.033 7.033 0 0 0-4.9308 2.0646 7.033 7.033 0 0 0-.1232 9.8068l2.395-2.395a3.6455 3.6455 0 0 1 5.1497-5.1478l2.397-2.3989a7.033 7.033 0 0 0-4.8877-1.9297zM7.07 4.9855a7.033 7.033 0 0 0-4.8878 1.9316l2.3911 2.3911a3.6434 3.6434 0 0 1 5.0227.1271l1.7341-2.9737-.0997-.0802A7.033 7.033 0 0 0 7.07 4.9855zm15.0093 2.1721l-2.3892 2.3911a3.6455 3.6455 0 0 1-5.1497 5.1497l-2.4067 2.4068a7.0362 7.0362 0 0 0 9.9456-9.9476zM1.932 7.1674a7.033 7.033 0 0 0-.002 9.6816l2.397-2.397a3.6434 3.6434 0 0 1-.004-4.8916zm7.664 7.4235c-1.38 1.3816-3.5863 1.411-5.0168.1134l-2.397 2.395c2.4693 2.3328 6.263 2.5753 9.0072.5455l.1368-.1115z" fill="white"></path></svg>
                        </span>
                        &nbsp; 
                        Colab Demo(TBD)
                    </a>
                </span>
                <span class="link-button">
                    <a class="link-button-content">
                        <span>
                            <svg class="svg-inline--fa fa-file-pdf fa-w-12" style="position: relative; top: 0.15em;" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg>
                        </span>
                        &nbsp;
                        中文版(待定)
                    </a>
                </span>
                <span class="link-button">
                    <a class="link-button-content">
                        <span>
                            <img src="../../images/gradio.svg" width="16"/>
                        </span>
                        Gradio Demo(TBD)
                    </a>
                </span>
                <span class="link-button">
                    <a class="link-button-content", href="#bib">
                        <span>
                            <svg class="svg-inline--fa fa-file-pdf" aria-hidden="true" style="position: relative; top: 0.15em;" width="1em" xmlns="http://www.w3.org/2000/svg" fill="currentColor" class="bi bi-bookmarks" viewBox="0 0 16 16"> <path fill="currentColor" fill-rule="evenodd" d="M2 4a2 2 0 0 1 2-2h6a2 2 0 0 1 2 2v11.5a.5.5 0 0 1-.777.416L7 13.101l-4.223 2.815A.5.5 0 0 1 2 15.5V4zm2-1a1 1 0 0 0-1 1v10.566l3.723-2.482a.5.5 0 0 1 .554 0L11 14.566V4a1 1 0 0 0-1-1H4z" fill="white"></path> <path fill="currentColor" fill-rule="evenodd" d="M4.268 1H12a1 1 0 0 1 1 1v11.768l.223.148A.5.5 0 0 0 14 13.5V2a2 2 0 0 0-2-2H6a2 2 0 0 0-1.732 1z" fill="white"></path> </svg>
                        </span>
                        &nbsp;  
                        BibTex
                    </a>
                </span>
            </div>
        </div>

<!--            <center><h1>Demo</h1></center>-->
<!--    <table align=center width=850px>-->
<!--        <tr>-->
<!--            <td>-->
<!--                Collecting MachineMem scores with the MachineMem measurer can be time-consuming. Thereby, we trained our MachineMem predictor and HumanMem predictor.-->
<!--                <br><B>Upload your image to see how memorable it is!</B>-->
<!--            <br>-->
<!--            </td>-->
<!--        </tr>-->
<!--    </table>-->
<!--    <br>-->
<!--    <script type="module"-->
<!--    src="https://gradio.s3-us-west-2.amazonaws.com/3.9.1/gradio.js">-->
<!--    </script>-->
<!--    <gradio-app space="Junlinh/memorability_prediction"></gradio-app>-->

        <!-- Visual Results -->
        <div style="background-color: #f5f5f5; margin-right: auto; margin-left: auto;">
            <div class="root-content" style="padding-top: 10px;">
                <h1 class="section-name">&#128293; Powerful Scene Text Removal Ability &#128293;</h1>
                <p class="section-content-text">
                    Our FETNet achieves superior scene text removal performance benefits from <strong>Feature Eraseing and Transferring (FET) mechanism </strong>
                    Here are some typical scene text removal cases.
                </p>
                <table style="border: 0px black solid; ">
                    <tbody>
                        <tr>
                            <td style="padding: 10px; width: 256;text-align: right;"><iframe frameborder="0" class="juxtapose" width="256" height="256" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=2fb12498-08c4-11ee-b5bd-6595d9b17862"></iframe></td>
                            <td style="padding: 10px; width: 256;text-align: right;"><iframe frameborder="0" class="juxtapose" width="256" height="256" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=343feac0-08c5-11ee-b5bd-6595d9b17862"></iframe></td>
                            <td style="padding: 10px; width: 256;text-align: right;"><iframe frameborder="0" class="juxtapose" width="256" height="256" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=0f19d2b4-08c6-11ee-b5bd-6595d9b17862"></iframe></td>
                            <td style="padding: 10px; width: 256;text-align: right;"><iframe frameborder="0" class="juxtapose" width="256" height="256" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=f5959c28-08c6-11ee-b5bd-6595d9b17862"></iframe></td>
                        </tr>
                        <tr>
                            <td style="padding: 10px; width: 256;text-align: right;"><iframe frameborder="0" class="juxtapose" width="256" height="256" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=46771c4a-08cf-11ee-b5bd-6595d9b17862"></iframe></td>
                            <td style="padding: 10px; width: 256;text-align: right;"><iframe frameborder="0" class="juxtapose" width="256" height="256" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=17a4ae46-08cf-11ee-b5bd-6595d9b17862"></iframe></td>
                            <td style="padding: 10px; width: 256;text-align: right;"><iframe frameborder="0" class="juxtapose" width="256" height="256" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=64cb4ee6-08cf-11ee-b5bd-6595d9b17862"></iframe></td>
                            <td style="padding: 10px; width: 256;text-align: right;"><iframe frameborder="0" class="juxtapose" width="256" height="256" src="https://cdn.knightlab.com/libs/juxtapose/latest/embed/index.html?uid=774ec02a-08cf-11ee-b5bd-6595d9b17862"></iframe></td>
                        </tr>
                    </tbody>
                </table>

            </div>
        </div>


        <!-- Paper Information -->
        <div class="root-content" style="padding-top: 30px;">
            <div class="section-content">
            <h1 class="section-name"> Abstract </h1>
                <p class="section-content-text">
   The scene text removal (STR) task aims to remove text regions and recover the background smoothly in images for private information protection. Most existing STR methods adopt encoder-decoder-based CNNs, with direct copies of the features in the skip connections. However, the encoded features contain both text texture and structure information. The insufficient utilization of text features hampers the performance of background reconstruction in text removal regions. To tackle these problems, we propose a novel Feature Erasing and Transferring (FET) mechanism to reconfigure the encoded features for STR in this paper. In FET, a Feature Erasing Module (FEM) is designed to erase text features. An attention module is responsible for generating the feature similarity guidance. The Feature Transferring Module (FTM) is introduced to transfer the corresponding features in different layers based on the attention guidance. With this mechanism, a one-stage, end-to-end trainable network called FETNet is constructed for scene text removal. In addition, to facilitate research on both scene text removal and segmentation tasks, we introduce a novel dataset, Flickr-ST, with multi-category annotations. A sufficient number of experiments and ablation studies are conducted on the public datasets and Flickr-ST. Our proposed method achieves state-of-the-art performance using most metrics, with remarkably higher quality scene text removal results. The source code of our work is available at: <a href="https://github.com/GuangtaoLyu/FETNet"> https://github.com/GuangtaoLyu/FETNet.</a>               </p>
            </div>

            <div style="padding-top: 20px;">
                <h1 class="section-name">Method</h1>
                <img src="images/model.jpg" width="1000"/>
                <p class="section-content-text">
    As shown in figure, the pipeline of our model consists of two parts: a generator $G$ and a discriminator $D$. In $G$, one simple encoder-decoder-skip-connection structure is constructed, consisting of five residual convolutional layers in the encoder and five residual convolutional (3$\times$3) layers in the decoder, respectively. Following the resnet backbone architecture design, the first to fifth layers of the encoder has kernel sizes of 7, 5, 3, 3, and 3 respectively. And the output feature maps of them have the size of $1, \frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{16}$ of the input image, respectively. In $D$, a four-layer CNN is designed to judge images as real or fake globally and locally.              </p>
            </div>

            <div style="padding-top: 20px;">
                <h1 class="section-name">Flickr_ST Dataset</h1>
                <img src="images/Flickr-ST.jpg" width="1000"/>
                <p class="section-content-text">
    The Flickr-ST dataset is a real-world dataset \cite{<a href=" https://arxiv.org/pdf/1705.02772.pdf"> Scene Text Eraser</a>} including 3,004 images with 2,204 images for training and 800 images for testing. The scene text in Flickr-ST has arbitrary orientations and shapes. Some examples are shown in Fig.(a). It provides five types of annotations, text removed images (in Fig.(b)), pixel-level text masks (in Fig.(c)), character instance segmentation labels, category labels, and character-level bounding box labels(in Fig.(d) and (e)). The word-level scene text regions can be calculated implicitly from those labels. To the best of our knowledge, Flickr-ST is the only dataset with such comprehensive annotations for scene text related tasks.           </div>

            <div style="padding-top: 20px;">
                <h1 class="section-name">
                    Visual Comparisons
                </h1>
                <img src="images/flickr_results.jpg" width="1000"/>
                <img src="images/scut_enstext_syn_results.jpg" width="1000" style="margin-top: -4px;"/>
            </div>

            <div style="padding-top: 20px;">
                <h1 class="section-name">Acknowledgements</h1>
                <p class="section-content-text"> This work is supported by the Open Project Program of the National Laboratory of Pattern Recognition (NLPR) (No.202200049).
            </div>

            <div>
                <h1 class="section-name" style="margin-top: 30px; text-align: left; font-size: 25px;">
                    BibTex
                </h1>
                <a name="bib"></a>
                <pre style="margin-top: 5px;" class="bibtex"><code>
@article{lyu2023fetnet,
  title={FETNet: Feature erasing and transferring network for scene text removal},
  author={Lyu, Guangtao and Liu, Kun and Zhu, Anna and Uchida, Seiichi and Iwana, Brian Kenji},
  journal={Pattern Recognition},
  volume={140},
  pages={109531},
  year={2023},
  publisher={Elsevier}
}</code></pre>
            </div>
            <div style="margin-bottom: 50px;">
                <h1 class="section-name" style="margin-top: 30px; margin-bottom: 10px; text-align: left; font-size: 25px;">
                    Contact
                </h1>
                <p class="section-content-text">

                    Feel free to contact us at <strong>Guangtao Lyu(吕光涛,<a href="mailto:1004392768@qq.com"> 1004392768@qq.com</a>) or Anna Zhu (<a href="mailto:annazhu@whut.edu.cn"> annazhu@whut.edu.cn</a>)</strong>
                     Personal page: <a href="https://guangtaolyu.github.io"> Guangtao Lyu(吕光涛)</a>
                </p>
            </div>
        </div>


        <div style="background-color: #f5f5f5; margin-right: auto; margin-left: auto; text-align: center; padding-top: 35px; padding-bottom: 35px;">
             <a href="https://www.freecounterstat.com" title="hit counter html"><img src="https://counter10.optistats.ovh/private/freecounterstat.php?c=w24ttp6jrxnr8ml9bsa5mdfkrffxmsf3" border="0" title="hit counter html" alt="hit counter html"></a>
            <p>Visitor Count</p>
        </div>
    </div>
</body>

<!--<script>-->
<!--    var img = document.getElementById('img');-->
<!--    var sliderbar = document.getElementById('slider');-->
<!--    slider.addEventListener('input', setImage);-->
<!--    var img_array = ['results_adjust/pumpkins_input_60.0.png',-->
<!--                      'results_adjust/pumpkins_input_50.0.png',-->
<!--                      'results_adjust/pumpkins_input_40.0.png',-->
<!--                      'results_adjust/pumpkins_input_30.0.png',-->
<!--                      'results_adjust/pumpkins_input_10.0.png',-->
<!--                      'results_adjust/pumpkins_input_20.0.png',-->
<!--                      'results_adjust/pumpkins_input_0.0.png',-->
<!--                      'results_adjust/pumpkins_input_-10.0.png',-->
<!--                      'results_adjust/pumpkins_input_-20.0.png',-->
<!--                      'results_adjust/pumpkins_input_-30.0.png',-->
<!--                      'results_adjust/pumpkins_input_-40.0.png',-->
<!--                      'results_adjust/pumpkins_input_-50.0.png',-->
<!--                      'results_adjust/pumpkins_input_-60.0.png',];-->
<!--    function setImage(e)-->
<!--    {-->
<!--        var {value, max} = e.target;-->
<!--        img.src = img_array[parseInt(value/20)];-->
<!--    }-->
<!--</script>-->

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$']]},
        messageStyle: "none"
    });
</script>
